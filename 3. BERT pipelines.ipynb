{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a76046-6efb-4724-b65b-9f376658b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04eef7c0-2dfc-48d7-867c-f9e0118a1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sentences\n",
    "txt = pd.read_csv('D:\\\\preprocessed_sentences_Lassy_Klein.csv')[['Category', 'Sentence', 'Length Label']]\n",
    "txt = list(txt.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e819252-6374-4c83-bce5-16f1432712aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings function\n",
    "def word_embeddings(model, tokenizer, wrangled_txt):\n",
    "    tokenized_txt = [(i[0], tokenizer.tokenize(i[1]), i[2]) for i in wrangled_txt]\n",
    "    ids_tokens = [(i[0], tokenizer.convert_tokens_to_ids(i[1]), i[2]) for i in tokenized_txt]\n",
    "    segment_ids = [(i[0], [1] * len(i[1]), i[2]) for i in tokenized_txt]\n",
    "    token_tensor = [(i[0], torch.tensor([i[1]]), i[2]) for i in ids_tokens]\n",
    "    segment_tensor = [(i[0], torch.tensor([i[1]]), i[2]) for i in segment_ids]\n",
    "    hidden_states_list = []\n",
    "    for i in range(len(token_tensor)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_tensor[i][1], segment_tensor[i][1])\n",
    "        hidden_states = outputs[-1]\n",
    "        hidden_states_list.append((token_tensor[i][0], hidden_states, token_tensor[i][2]))\n",
    "    token_embeddings_list = []\n",
    "    for h_s in hidden_states_list:\n",
    "        token_embeddings = torch.stack(h_s[1], dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1) # remove the batch dimension\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        token_embeddings_list.append((h_s[0], token_embeddings, h_s[2]))\n",
    "    word_embeddings = []\n",
    "    for t_e in token_embeddings_list:\n",
    "        token_vectors_sum = []\n",
    "        for token in t_e[1]:\n",
    "            sum_vector = torch.sum(token[-4:], dim=0) # sum the last 4 hidden layers\n",
    "            token_vectors_sum.append(sum_vector)\n",
    "        word_embeddings.append((t_e[0], token_vectors_sum, t_e[2]))\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba33fd8-cd1f-4d6b-a1ab-690bada2e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings function\n",
    "def word_embedding_to_sentence_embedding(word_embedding):\n",
    "    sentence_embedding = []\n",
    "    for i in range(len(word_embedding[1][0])):\n",
    "        total = 0\n",
    "        for j in word_embedding[1]:\n",
    "            total = torch.add(total, j[i])\n",
    "        average = total / len(word_embedding[1])\n",
    "        sentence_embedding.append(average.item())\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ec2f9-607f-4858-a258-a9d1e18487af",
   "metadata": {},
   "source": [
    "# RobBERT-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ced9de-e5f4-440f-b796-2c0936c6d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50000, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, model\n",
    "robbert_tokenizer = AutoTokenizer.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-large\")\n",
    "robbert_model = AutoModel.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-large\", output_hidden_states = True, return_dict = False)\n",
    "robbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fde10d-e3b0-489d-a931-e7beffb4216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "wrangled_txt = [(i[0], '<s> ' + i[1], i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21d2105-0091-4ed0-8acc-8b92e07f377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_robbert = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c982cde-3d99-4fea-b1a7-6a99498968f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings\n",
    "sentence_embeddings_robbert = []\n",
    "for word_embedding in word_embeddings_robbert:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert.append((word_embedding[0], sentence_embedding, word_embedding[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd12f45-ffcd-4a00-ad94-ec18c68e5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robbert = pd.DataFrame(sentence_embeddings_robbert, columns=['Category', 'Sentence Embedding', 'Length Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4d0f5-0d4a-4401-a0ab-ea8233f30342",
   "metadata": {},
   "source": [
    "# BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551892ff-6d18-4311-87dd-6b59361ae970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30073, 768, padding_idx=3)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, model\n",
    "bertje_tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "bertje_model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\", output_hidden_states = True, return_dict = False)\n",
    "bertje_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26979bb2-c0d1-4638-8023-418453003ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "wrangled_txt = [(i[0], '[CLS] ' + i[1] + ' [SEP]', i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c27fa8-7ca1-4320-b3cd-2898700b71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings BERTje\n",
    "word_embeddings_bertje = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "736ea75f-aa12-48cb-94ef-27e6eb1530d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings BERTje\n",
    "sentence_embeddings_bertje = []\n",
    "for word_embedding in word_embeddings_bertje:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje.append((word_embedding[0], sentence_embedding, word_embedding[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6c1031e-9e30-4d5a-986c-43c41883fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bertje = pd.DataFrame(sentence_embeddings_bertje, columns=['Category', 'Sentence Embedding', 'Length Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554feeb-0da4-4b1a-83ed-0e6fc07b300f",
   "metadata": {},
   "source": [
    "# EuroBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c65fba-7474-4834-baee-7ccf0ec94cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EuroBertModel(\n",
       "  (embed_tokens): Embedding(128256, 768, padding_idx=128001)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EuroBertDecoderLayer(\n",
       "      (self_attn): EuroBertAttention(\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (mlp): EuroBertMLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): EuroBertRMSNorm((768,), eps=1e-05)\n",
       "      (post_attention_layernorm): EuroBertRMSNorm((768,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): EuroBertRMSNorm((768,), eps=1e-05)\n",
       "  (rotary_emb): EuroBertRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, model\n",
    "eurobert_tokenizer = AutoTokenizer.from_pretrained(\"EuroBERT/EuroBERT-210m\")\n",
    "eurobert_model = AutoModel.from_pretrained(\"EuroBERT/EuroBERT-210m\", trust_remote_code=True, output_hidden_states = True, return_dict = False)\n",
    "eurobert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfd70c19-5fa7-41a9-b4e1-3bacc1e75457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "wrangled_txt = [(i[0], '[CLS] ' + i[1] + ' [SEP]', i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d18c723-e248-4504-853f-62683222fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings EuroBERT\n",
    "word_embeddings_eurobert = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23d2eb03-f3d6-4065-b4d7-8652b595cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings EuroBERT\n",
    "sentence_embeddings_eurobert = []\n",
    "for word_embedding in word_embeddings_eurobert:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert.append((word_embedding[0], sentence_embedding, word_embedding[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "882ddad9-643c-40fa-a1e7-7952367e1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "df_eurobert = pd.DataFrame(sentence_embeddings_eurobert, columns=['Category', 'Sentence Embedding', 'Length Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45084b3-3ea7-4ed4-ad13-88d1ae5322f8",
   "metadata": {},
   "source": [
    "# mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3701354c-63da-4a7a-baf3-b0e222277171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word embeddings mBERT\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "mbert_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\", output_hidden_states = True, return_dict = False)\n",
    "mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8000217-9fb0-41b7-bf62-43e40f4389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt = [(i[0], '[CLS] ' + i[1] + ' [SEP]', i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad5081c2-6b92-4458-b874-33f6559bcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_mbert = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64c07930-eabe-45c2-83cb-bec8368b4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings mBERT\n",
    "sentence_embeddings_mbert = []\n",
    "for word_embedding in word_embeddings_mbert:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert.append((word_embedding[0], sentence_embedding, word_embedding[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa11e1ca-88c7-48d7-8627-6e2bdd740f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbert = pd.DataFrame(sentence_embeddings_mbert, columns=['Category', 'Sentence Embedding', 'Length Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8927d3-07aa-4ddb-875c-dfc98896e49d",
   "metadata": {},
   "source": [
    "# Saving the sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46878014-6886-4e9d-baf8-62aec3a67395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv files\n",
    "# robBERT\n",
    "df_robbert.to_csv('D:\\\\sentence_embeddings_robbert_lassy_klein.csv')\n",
    "\n",
    "# BERTje\n",
    "df_bertje.to_csv('D:\\\\sentence_embeddings_bertje_lassy_klein.csv')\n",
    "\n",
    "# EuroBERT\n",
    "df_eurobert.to_csv('D:\\\\sentence_embeddings_eurobert_lassy_klein.csv')\n",
    "\n",
    "# mBERT\n",
    "df_mbert.to_csv('D:\\\\sentence_embeddings_mbert_lassy_klein.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
