{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a76046-6efb-4724-b65b-9f376658b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04eef7c0-2dfc-48d7-867c-f9e0118a1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in preprocessed sentences\n",
    "txt = pd.read_csv('D:\\\\preprocessed_sentences.csv')[['Category', 'Sentence', 'Length Label']]\n",
    "txt = list(txt.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e819252-6374-4c83-bce5-16f1432712aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings function\n",
    "def word_embeddings(model, tokenizer, wrangled_txt):\n",
    "    tokenized_txt = [(i[0], tokenizer.tokenize(i[1]), i[2]) for i in wrangled_txt]\n",
    "    ids_tokens = [(i[0], tokenizer.convert_tokens_to_ids(i[1]), i[2]) for i in tokenized_txt]\n",
    "    segment_ids = [(i[0], [1] * len(i[1]), i[2]) for i in tokenized_txt]\n",
    "    token_tensor = [(i[0], torch.tensor([i[1]]), i[2]) for i in ids_tokens]\n",
    "    segment_tensor = [(i[0], torch.tensor([i[1]]), i[2]) for i in segment_ids]\n",
    "    hidden_states_list = []\n",
    "    for i in range(len(token_tensor)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_tensor[i][1], segment_tensor[i][1])\n",
    "        hidden_states = outputs[-1]\n",
    "        hidden_states_list.append((token_tensor[i][0], hidden_states, token_tensor[i][2]))\n",
    "    token_embeddings_list = []\n",
    "    for h_s in hidden_states_list:\n",
    "        token_embeddings = torch.stack(h_s[1], dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1) # remove the batch dimension\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        token_embeddings_list.append((h_s[0], token_embeddings, h_s[2]))\n",
    "    word_embeddings = []\n",
    "    for t_e in token_embeddings_list:\n",
    "        token_vectors_sum = []\n",
    "        for token in t_e[1]:\n",
    "            sum_vector = torch.sum(token[-4:], dim=0) # sum the last 4 hidden layers\n",
    "            token_vectors_sum.append(sum_vector)\n",
    "        word_embeddings.append((t_e[0], token_vectors_sum, t_e[2]))\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba33fd8-cd1f-4d6b-a1ab-690bada2e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings function\n",
    "def word_embedding_to_sentence_embedding(word_embedding):\n",
    "    sentence_embedding = []\n",
    "    for i in range(len(word_embedding[1][0])):\n",
    "        total = 0\n",
    "        for j in word_embedding[1]:\n",
    "            total = torch.add(total, j[i])\n",
    "        average = total / len(word_embedding[1])\n",
    "        sentence_embedding.append(average.item())\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ec2f9-607f-4858-a258-a9d1e18487af",
   "metadata": {},
   "source": [
    "# RobBERT-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ced9de-e5f4-440f-b796-2c0936c6d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50000, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, model\n",
    "robbert_tokenizer = AutoTokenizer.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-large\")\n",
    "robbert_model = AutoModel.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-large\", output_hidden_states = True, return_dict = False)\n",
    "robbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59fde10d-e3b0-489d-a931-e7beffb4216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7935"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "wrangled_txt = [(i[0], '<s> ' + i[1], i[2]) for i in txt]\n",
    "len(wrangled_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "802b9f85-31a4-4f69-b2f4-b40bd7a4a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_1 = wrangled_txt[:1000]\n",
    "word_embeddings_robbert_1 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_1)\n",
    "sentence_embeddings_robbert_1 = []\n",
    "for word_embedding in word_embeddings_robbert_1:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_1.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_1 = pd.DataFrame(sentence_embeddings_robbert_1, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_1.to_csv('D:\\\\robbert_embeddings_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad8c0260-a801-42f2-aaba-578b6730ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_2 = wrangled_txt[1000:2000]\n",
    "word_embeddings_robbert_2 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_2)\n",
    "sentence_embeddings_robbert_2 = []\n",
    "for word_embedding in word_embeddings_robbert_2:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_2.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_2 = pd.DataFrame(sentence_embeddings_robbert_2, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_2.to_csv('D:\\\\robbert_embeddings_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d9eee9e-de1d-484f-b349-d40963fe533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_3 = wrangled_txt[2000:3000]\n",
    "word_embeddings_robbert_3 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_3)\n",
    "sentence_embeddings_robbert_3 = []\n",
    "for word_embedding in word_embeddings_robbert_3:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_3.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_3 = pd.DataFrame(sentence_embeddings_robbert_3, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_3.to_csv('D:\\\\robbert_embeddings_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3637735-6ff4-4289-a9ff-48a923177df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_4 = wrangled_txt[3000:4000]\n",
    "word_embeddings_robbert_4 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_4)\n",
    "sentence_embeddings_robbert_4 = []\n",
    "for word_embedding in word_embeddings_robbert_4:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_4.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_4 = pd.DataFrame(sentence_embeddings_robbert_4, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_4.to_csv('D:\\\\robbert_embeddings_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e1c5dfd-aed3-430d-908c-90417908ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_5 = wrangled_txt[4000:5000]\n",
    "word_embeddings_robbert_5 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_5)\n",
    "sentence_embeddings_robbert_5 = []\n",
    "for word_embedding in word_embeddings_robbert_5:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_5.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_5 = pd.DataFrame(sentence_embeddings_robbert_5, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_5.to_csv('D:\\\\robbert_embeddings_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83e57b1f-dd3a-4a3b-8f53-b2abacc305ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_6 = wrangled_txt[5000:6000]\n",
    "word_embeddings_robbert_6 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_6)\n",
    "sentence_embeddings_robbert_6 = []\n",
    "for word_embedding in word_embeddings_robbert_6:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_6.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_6 = pd.DataFrame(sentence_embeddings_robbert_6, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_6.to_csv('D:\\\\robbert_embeddings_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "678d8a21-6397-4d48-94dc-27ccd23f424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_7 = wrangled_txt[6000:7000]\n",
    "word_embeddings_robbert_7 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_7)\n",
    "sentence_embeddings_robbert_7 = []\n",
    "for word_embedding in word_embeddings_robbert_7:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_7.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_7 = pd.DataFrame(sentence_embeddings_robbert_7, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_7.to_csv('D:\\\\robbert_embeddings_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6892ad06-a991-4276-a58e-0d175bd9e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_8 = wrangled_txt[7000:]\n",
    "word_embeddings_robbert_8 = word_embeddings(robbert_model, robbert_tokenizer, wrangled_txt_8)\n",
    "sentence_embeddings_robbert_8 = []\n",
    "for word_embedding in word_embeddings_robbert_8:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_robbert_8.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_robbert_8 = pd.DataFrame(sentence_embeddings_robbert_8, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_robbert_8.to_csv('D:\\\\robbert_embeddings_8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4d0f5-0d4a-4401-a0ab-ea8233f30342",
   "metadata": {},
   "source": [
    "# BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "551892ff-6d18-4311-87dd-6b59361ae970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30073, 768, padding_idx=3)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, model\n",
    "bertje_tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "bertje_model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\", output_hidden_states = True, return_dict = False)\n",
    "bertje_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26979bb2-c0d1-4638-8023-418453003ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "wrangled_txt = [(i[0], '[CLS] ' + i[1] + ' [SEP]', i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30c2c97f-89de-4173-8955-e8814c1ab083",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_1 = wrangled_txt[:1000]\n",
    "word_embeddings_bertje_1 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_1)\n",
    "sentence_embeddings_bertje_1 = []\n",
    "for word_embedding in word_embeddings_bertje_1:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_1.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_1 = pd.DataFrame(sentence_embeddings_bertje_1, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_1.to_csv('D:\\\\bertje_embeddings_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "233f8f7c-6dd6-4bfb-8e4b-9b63eb096284",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_2 = wrangled_txt[1000:2000]\n",
    "word_embeddings_bertje_2 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_2)\n",
    "sentence_embeddings_bertje_2 = []\n",
    "for word_embedding in word_embeddings_bertje_2:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_2.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_2 = pd.DataFrame(sentence_embeddings_bertje_2, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_2.to_csv('D:\\\\bertje_embeddings_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67ccc00a-fc30-4501-88e7-7bd8dfefceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_3 = wrangled_txt[2000:3000]\n",
    "word_embeddings_bertje_3 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_3)\n",
    "sentence_embeddings_bertje_3 = []\n",
    "for word_embedding in word_embeddings_bertje_3:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_3.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_3 = pd.DataFrame(sentence_embeddings_bertje_3, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_3.to_csv('D:\\\\bertje_embeddings_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24606125-e7b7-4cf4-bfcd-a42850c635e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_4 = wrangled_txt[3000:4000]\n",
    "word_embeddings_bertje_4 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_4)\n",
    "sentence_embeddings_bertje_4 = []\n",
    "for word_embedding in word_embeddings_bertje_4:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_4.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_4 = pd.DataFrame(sentence_embeddings_bertje_4, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_4.to_csv('D:\\\\bertje_embeddings_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "922a853f-5696-46cf-9b80-3cc0b3b65e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_5 = wrangled_txt[4000:5000]\n",
    "word_embeddings_bertje_5 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_5)\n",
    "sentence_embeddings_bertje_5 = []\n",
    "for word_embedding in word_embeddings_bertje_5:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_5.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_5 = pd.DataFrame(sentence_embeddings_bertje_5, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_5.to_csv('D:\\\\bertje_embeddings_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a2796db-07c5-42ef-b5c9-84a071a5983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_6 = wrangled_txt[5000:6000]\n",
    "word_embeddings_bertje_6 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_6)\n",
    "sentence_embeddings_bertje_6 = []\n",
    "for word_embedding in word_embeddings_bertje_6:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_6.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_6 = pd.DataFrame(sentence_embeddings_bertje_6, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_6.to_csv('D:\\\\bertje_embeddings_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5b0ff20-ce6a-404c-9198-d66e82bad5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_7 = wrangled_txt[6000:7000]\n",
    "word_embeddings_bertje_7 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_7)\n",
    "sentence_embeddings_bertje_7 = []\n",
    "for word_embedding in word_embeddings_bertje_7:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_7.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_7 = pd.DataFrame(sentence_embeddings_bertje_7, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_7.to_csv('D:\\\\bertje_embeddings_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cab5d7e-d16e-45c1-9770-29444256f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_8 = wrangled_txt[7000:]\n",
    "word_embeddings_bertje_8 = word_embeddings(bertje_model, bertje_tokenizer, wrangled_txt_8)\n",
    "sentence_embeddings_bertje_8 = []\n",
    "for word_embedding in word_embeddings_bertje_8:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_bertje_8.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_bertje_8 = pd.DataFrame(sentence_embeddings_bertje_8, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_bertje_8.to_csv('D:\\\\bertje_embeddings_8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554feeb-0da4-4b1a-83ed-0e6fc07b300f",
   "metadata": {},
   "source": [
    "# EuroBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9c65fba-7474-4834-baee-7ccf0ec94cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EuroBertModel(\n",
       "  (embed_tokens): Embedding(128256, 768, padding_idx=128001)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EuroBertDecoderLayer(\n",
       "      (self_attn): EuroBertAttention(\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (mlp): EuroBertMLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): EuroBertRMSNorm((768,), eps=1e-05)\n",
       "      (post_attention_layernorm): EuroBertRMSNorm((768,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): EuroBertRMSNorm((768,), eps=1e-05)\n",
       "  (rotary_emb): EuroBertRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, model\n",
    "eurobert_tokenizer = AutoTokenizer.from_pretrained(\"EuroBERT/EuroBERT-210m\")\n",
    "eurobert_model = AutoModel.from_pretrained(\"EuroBERT/EuroBERT-210m\", trust_remote_code=True, output_hidden_states = True, return_dict = False)\n",
    "eurobert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfd70c19-5fa7-41a9-b4e1-3bacc1e75457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "wrangled_txt = [(i[0], '[CLS] ' + i[1] + ' [SEP]', i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eda4cfdf-d92a-4c49-878a-1e4ccfc9227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_1 = wrangled_txt[:1000]\n",
    "word_embeddings_eurobert_1 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_1)\n",
    "sentence_embeddings_eurobert_1 = []\n",
    "for word_embedding in word_embeddings_eurobert_1:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_1.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_1 = pd.DataFrame(sentence_embeddings_eurobert_1, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_1.to_csv('D:\\\\eurobert_embeddings_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f96be8f3-f06f-4752-8366-69382b21a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_2 = wrangled_txt[1000:2000]\n",
    "word_embeddings_eurobert_2 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_2)\n",
    "sentence_embeddings_eurobert_2 = []\n",
    "for word_embedding in word_embeddings_eurobert_2:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_2.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_2 = pd.DataFrame(sentence_embeddings_eurobert_2, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_2.to_csv('D:\\\\eurobert_embeddings_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d87b3bab-bc59-4cc6-96ef-696cd9a1c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_3 = wrangled_txt[2000:3000]\n",
    "word_embeddings_eurobert_3 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_3)\n",
    "sentence_embeddings_eurobert_3 = []\n",
    "for word_embedding in word_embeddings_eurobert_3:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_3.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_3 = pd.DataFrame(sentence_embeddings_eurobert_3, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_3.to_csv('D:\\\\eurobert_embeddings_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b102f4b0-5ec8-47d2-a392-e830657cc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_4 = wrangled_txt[3000:4000]\n",
    "word_embeddings_eurobert_4 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_4)\n",
    "sentence_embeddings_eurobert_4 = []\n",
    "for word_embedding in word_embeddings_eurobert_4:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_4.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_4 = pd.DataFrame(sentence_embeddings_eurobert_4, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_4.to_csv('D:\\\\eurobert_embeddings_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aaf9e13b-ff64-433c-8d1b-1eea92092c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_5 = wrangled_txt[4000:5000]\n",
    "word_embeddings_eurobert_5 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_5)\n",
    "sentence_embeddings_eurobert_5 = []\n",
    "for word_embedding in word_embeddings_eurobert_5:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_5.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_5 = pd.DataFrame(sentence_embeddings_eurobert_5, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_5.to_csv('D:\\\\eurobert_embeddings_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "971ccfcf-9d20-43ab-a724-ad31c7eae937",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_6 = wrangled_txt[5000:6000]\n",
    "word_embeddings_eurobert_6 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_6)\n",
    "sentence_embeddings_eurobert_6 = []\n",
    "for word_embedding in word_embeddings_eurobert_6:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_6.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_6 = pd.DataFrame(sentence_embeddings_eurobert_6, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_6.to_csv('D:\\\\eurobert_embeddings_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14e36c04-303f-4253-9da5-3f48d3bda0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_7 = wrangled_txt[6000:7000]\n",
    "word_embeddings_eurobert_7 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_7)\n",
    "sentence_embeddings_eurobert_7 = []\n",
    "for word_embedding in word_embeddings_eurobert_7:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_7.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_7 = pd.DataFrame(sentence_embeddings_eurobert_7, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_7.to_csv('D:\\\\eurobert_embeddings_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92efe8ca-4bca-4ba9-acc3-c272e0513ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_8 = wrangled_txt[7000:]\n",
    "word_embeddings_eurobert_8 = word_embeddings(eurobert_model, eurobert_tokenizer, wrangled_txt_8)\n",
    "sentence_embeddings_eurobert_8 = []\n",
    "for word_embedding in word_embeddings_eurobert_8:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_eurobert_8.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_eurobert_8 = pd.DataFrame(sentence_embeddings_eurobert_8, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_eurobert_8.to_csv('D:\\\\eurobert_embeddings_8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45084b3-3ea7-4ed4-ad13-88d1ae5322f8",
   "metadata": {},
   "source": [
    "# mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3701354c-63da-4a7a-baf3-b0e222277171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word embeddings mBERT\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "mbert_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\", output_hidden_states = True, return_dict = False)\n",
    "mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8000217-9fb0-41b7-bf62-43e40f4389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt = [(i[0], '[CLS] ' + i[1] + ' [SEP]', i[2]) for i in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00626cdb-b7d1-4a45-a0ea-8fecaf107315",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_1 = wrangled_txt[:1000]\n",
    "word_embeddings_mbert_1 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_1)\n",
    "sentence_embeddings_mbert_1 = []\n",
    "for word_embedding in word_embeddings_mbert_1:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_1.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_1 = pd.DataFrame(sentence_embeddings_mbert_1, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_1.to_csv('D:\\\\mbert_embeddings_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "26318210-c0b6-4d7e-a5d2-faf5fce4d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_2 = wrangled_txt[1000:2000]\n",
    "word_embeddings_mbert_2 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_2)\n",
    "sentence_embeddings_mbert_2 = []\n",
    "for word_embedding in word_embeddings_mbert_2:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_2.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_2 = pd.DataFrame(sentence_embeddings_mbert_2, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_2.to_csv('D:\\\\mbert_embeddings_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d273510f-75f7-4004-a900-e536f6eefea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_3 = wrangled_txt[2000:3000]\n",
    "word_embeddings_mbert_3 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_3)\n",
    "sentence_embeddings_mbert_3 = []\n",
    "for word_embedding in word_embeddings_mbert_3:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_3.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_3 = pd.DataFrame(sentence_embeddings_mbert_3, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_3.to_csv('D:\\\\mbert_embeddings_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10372684-ce90-4680-8bb8-4fea5bc5d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_4 = wrangled_txt[3000:4000]\n",
    "word_embeddings_mbert_4 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_4)\n",
    "sentence_embeddings_mbert_4 = []\n",
    "for word_embedding in word_embeddings_mbert_4:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_4.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_4 = pd.DataFrame(sentence_embeddings_mbert_4, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_4.to_csv('D:\\\\mbert_embeddings_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37733b95-2e92-4280-bad4-e6835cd062a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_5 = wrangled_txt[4000:5000]\n",
    "word_embeddings_mbert_5 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_5)\n",
    "sentence_embeddings_mbert_5 = []\n",
    "for word_embedding in word_embeddings_mbert_5:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_5.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_5 = pd.DataFrame(sentence_embeddings_mbert_5, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_5.to_csv('D:\\\\mbert_embeddings_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6041464-3164-4cd3-bc03-35c3043952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_6 = wrangled_txt[5000:6000]\n",
    "word_embeddings_mbert_6 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_6)\n",
    "sentence_embeddings_mbert_6 = []\n",
    "for word_embedding in word_embeddings_mbert_6:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_6.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_6 = pd.DataFrame(sentence_embeddings_mbert_6, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_6.to_csv('D:\\\\mbert_embeddings_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0d4919a-854c-48d3-9961-661cd27bb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_7 = wrangled_txt[6000:7000]\n",
    "word_embeddings_mbert_7 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_7)\n",
    "sentence_embeddings_mbert_7 = []\n",
    "for word_embedding in word_embeddings_mbert_7:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_7.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_7 = pd.DataFrame(sentence_embeddings_mbert_7, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_7.to_csv('D:\\\\mbert_embeddings_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6f3c12e-71c2-4156-b426-8079e901017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_txt_8 = wrangled_txt[7000:]\n",
    "word_embeddings_mbert_8 = word_embeddings(mbert_model, mbert_tokenizer, wrangled_txt_8)\n",
    "sentence_embeddings_mbert_8 = []\n",
    "for word_embedding in word_embeddings_mbert_8:\n",
    "    sentence_embedding = word_embedding_to_sentence_embedding(word_embedding)\n",
    "    sentence_embeddings_mbert_8.append((word_embedding[0], sentence_embedding, word_embedding[2]))\n",
    "df_mbert_8 = pd.DataFrame(sentence_embeddings_mbert_8, columns=['Category', 'Sentence Embedding', 'Length Label'])\n",
    "df_mbert_8.to_csv('D:\\\\mbert_embeddings_8.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
